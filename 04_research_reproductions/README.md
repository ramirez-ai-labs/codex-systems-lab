# Paper Reproduction: Edit Distance vs Acceptance Rate

## ðŸ“„ What This Experiment Is About

This folder reproduces a **core finding from AI-assisted coding research**:

> **The closer an AI-generated code suggestion is to the final accepted code,  
> the more likely a developer is to accept it.**

That â€œclosenessâ€ is measured using **edit distance** â€” a simple but powerful
string-based metric.

This experiment helps answer:
- Why small improvements in code quality matter
- Why â€œalmost rightâ€ suggestions are often accepted
- How evaluation metrics connect to real developer behavior

---

## ðŸ§  Key Concepts (Beginner-Friendly)

### What is *Edit Distance*?

Edit distance measures **how many small changes** are needed to turn one string
into another.

Examples:
- `foo()` â†’ `foo()` â†’ edit distance = **0**
- `foo()` â†’ `bar()` â†’ edit distance = **3**
- One deletion, insertion, or replacement = **1 edit**

In code generation:
- Low edit distance â†’ model output is very close to what the developer wants
- High edit distance â†’ developer has to rewrite a lot â†’ likely rejected

---

### What is *Acceptance Rate*?

Acceptance rate is the fraction of suggestions that developers **keep** instead
of deleting or rewriting.

Example:
- 100 suggestions shown
- 65 accepted
â†’ acceptance rate = **65%**

---

## ðŸ”¬ What This Reproduction Demonstrates

We recreate the relationship:

| Edit Distance | Acceptance Likelihood |
|--------------|-----------------------|
| Very low     | Very high             |
| Medium       | Mixed                 |
| High         | Very low              |

This mirrors real findings from:
- GitHub Copilot studies
- Codex productivity research
- IDE telemetry analyses

---

## ðŸ§ª Files in This Folder

```bash
paper_1_edit_distance_vs_acceptance/
â”œâ”€â”€ README.md â† You are here
â”œâ”€â”€ replicate_experiment.ipynb â† Step-by-step experiment
â””â”€â”€ RESULTS.md â† Findings and interpretation
```

---

## ðŸ““ `replicate_experiment.ipynb`

The notebook walks through:

1. Creating **synthetic code suggestions**
2. Simulating **developer-edited final code**
3. Computing **edit distance**
4. Mapping distance â†’ acceptance probability
5. Visualizing the relationship

Everything is:
- Small
- CPU-friendly
- Heavily commented
- Designed for learning, not scale

---

## ðŸ“Š Expected Outcome (Intuition)

You should see:
- Acceptance rate drops sharply as edit distance increases
- Most â€œwinsâ€ come from **small improvements**
- Diminishing returns after a certain quality threshold

This explains why:
- Minor model improvements can produce big DX gains
- Perfect code is not required for usefulness
- Evaluation should focus on *closeness*, not perfection

---

## ðŸ§© Why This Matters for AI Coding Systems

This experiment connects **metrics â†’ human behavior**:

- Edit distance â†’ developer effort
- Developer effort â†’ acceptance
- Acceptance â†’ productivity gains

Modern systems use this insight to:
- Tune decoding strategies
- Rank suggestions
- Decide when to show or hide completions

---

## âš ï¸ Important Notes

- This is a **conceptual reproduction**, not an exact paper replication
- Data is synthetic to keep the lab lightweight
- The goal is understanding, not leaderboard performance

---

## ðŸŽ“ Beginner Takeaway

> AI coding models donâ€™t need to be perfect.  
> They need to be *close enough* to save time.

Edit distance is one of the simplest ways to measure that closeness.

---

## ðŸ”— Next Steps

After this experiment, consider:
- Comparing edit distance to semantic similarity
- Adding latency or retry cost into acceptance modeling
- Connecting this to RAG quality or prompt strategies

When youâ€™re ready, move on to:

```bash
paper_2_rag_scaling_laws/
```
